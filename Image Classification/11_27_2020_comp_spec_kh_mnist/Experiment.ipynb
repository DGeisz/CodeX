{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompSpec Krotov Hopfield MNIST Benchmark\n",
    "\n",
    "## Intro\n",
    "* **Date**: 11/27/2020\n",
    "* **What**: This experiment is going to benchmark CompSpec against the unsupervised network in [Krotov's and Hopfield's paper](https://www.pnas.org/content/116/16/7723).  So basically, I'm going to train a CompSpec layer, and then build a tf layer on top of my network to do the final classification.  Then, of course, I need to train a full DNN of the same architecture and see how my network measures up.  \n",
    "* **Why**: CompSpec is crazy fast, and crazy efficient at training.  This we know and love.  But we need to see how good it actually is at the classification task from Hopfield and Krotov's paper.  I'm thinking CompSpec actually trains even faster than backprop, which is absolutely fucking wild.  But we'll see.\n",
    "* **Hopes**: I hope that training the CompSpec layer on a single epoch of data matches the performance of backprop after several epochs.  I'm fairly certain CompSpec is more efficient than backprop, so if that's the case, I've effectively found an architecture that's better than the standard training algorithm for machine learning.\n",
    "* **Limitations**: If KH's paper is to be believed, then the non-charged CompSpec might hit an upper limit in performance.  Apparently that's what happened for them when their network only learned image prototypes.  But don't you worry, I'll also try the benchmark out with charged CompSpec and see if that helps things a lot.  I might even throw in a *novel* (ooh hoo hoo!) version of charged CompSpec that I thought of last night.  Here we go!\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "\n",
    "L = 28 * 28   #Size of mnist in pixels\n",
    "S = 60000     #Size of training set\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X = train_X / 255.0\n",
    "test_X = test_X / 255.0\n",
    "\n",
    "flat_x = np.reshape(train_X, [-1, L])\n",
    "flat_test = np.reshape(test_X, [-1, L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky):\n",
    "    yy=0\n",
    "    HM=np.zeros((28*Ky,28*Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*28:(y+1)*28,x*28:(x+1)*28]=synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    plt.clf()\n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "    fig.colorbar(im,ticks=[0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "flat_x: training data\n",
    "S: Size of training set\n",
    "L: Size of input\n",
    "Kx: Num cols of neurons\n",
    "Ky: Num rows of neurons\n",
    "Nep: Num epochs\n",
    "T_s: Number of training inputs\n",
    "xi: Base learning constant\n",
    "phi: Specialization ema constant\n",
    "B: Batch size\n",
    "\n",
    "Returns: (synapse_weights, neuron specialization values)\n",
    "\"\"\"\n",
    "def comp_spec_mb(flat_x, S, L, Kx, Ky, Nep, T_s, xi, phi, B):\n",
    "    start = time()\n",
    "    N = Kx * Ky\n",
    "    \n",
    "    w = np.abs(np.random.normal(0, 1, (N, L))) # synapses of each neuron\n",
    "    w = w / np.array([np.linalg.norm(w, axis=1)]).T #NORMALIZE THE WEIGHTS TO PREVENT EXXXPLOSIONS\n",
    "    s = np.zeros(N).reshape(-1, 1) # Specialization for each neuron+\n",
    "\n",
    "    for ep in range(Nep):\n",
    "        # Uncomment the following line if you'd like to shuffle the data between epochs\n",
    "        inputs = flat_x[np.random.permutation(S), :].reshape(S, L)\n",
    "\n",
    "        for i in tqdm(range(T_s // B)):\n",
    "            v = inputs[i * B: (i + 1) * B, :]\n",
    "\n",
    "            w_mul_v = w @ v.T \n",
    "            o = w_mul_v / (np.linalg.norm(w, axis=1).reshape(-1, 1) * np.linalg.norm(v, axis=1))\n",
    "\n",
    "            c = ((1 - s) ** 2) / (1 - o)\n",
    "\n",
    "            wins = np.argmax(c, axis=0)     \n",
    "\n",
    "            win_mask = np.zeros((N, B))\n",
    "            win_mask[wins, np.arange(B)] = 1\n",
    "            win_mask = (win_mask / np.maximum(np.sum(win_mask, axis=1), 1).reshape(-1, 1))\n",
    "\n",
    "            win_avg = (np.sum(w_mul_v * win_mask, axis=1)).reshape(-1, 1)\n",
    "\n",
    "            v_update = win_mask @ v\n",
    "\n",
    "            del_syn = (v_update - (win_avg * w)) * (((1 - s) ** 2) + 0.1) * xi\n",
    "\n",
    "            w += del_syn\n",
    "\n",
    "            s[wins] *= (1 - phi)\n",
    "            s += phi * np.sum(o * win_mask, axis=1).reshape(-1, 1)\n",
    "\n",
    "    print(\"Max val: \", np.amax(s), \"Min value: \", np.amin(s), \"Mean val: \", np.mean(s), \"Std: \", np.std(s))\n",
    "    print(\"Elapsed time: \", time() - start, \" seconds\")\n",
    "    \n",
    "    return (w, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "flat_x: training data\n",
    "S: Size of training set\n",
    "L: Size of input\n",
    "Kx: Num cols of neurons\n",
    "Ky: Num rows of neurons\n",
    "Nep: Num epochs\n",
    "T_s: Number of training inputs\n",
    "xi: Base learning constant\n",
    "phi: Specialization ema constant\n",
    "k: Rank of the repelled neuron\n",
    "delta: local repulsion constant (should be less than 1)\n",
    "B: Batch size\n",
    "\n",
    "Returns: (synapse_weights, neuron specialization values)\n",
    "\"\"\"\n",
    "def charged_comp_spec_mb(flat_x, S, L, Kx, Ky, Nep, T_s, xi, phi, delta, k, B):\n",
    "    start = time()\n",
    "    N = Kx * Ky\n",
    "    \n",
    "    w = np.abs(np.random.normal(0, 1, (N, L))) # synapses of each neuron\n",
    "    w = w / np.array([np.linalg.norm(w, axis=1)]).T #NORMALIZE THE WEIGHTS TO PREVENT EXXXPLOSIONS\n",
    "    s = np.zeros(N).reshape(-1, 1) # Specialization for each neuron+\n",
    "\n",
    "    for ep in range(Nep):\n",
    "        # Uncomment the following line if you'd like to shuffle the data between epochs\n",
    "        inputs = flat_x[np.random.permutation(S), :]\n",
    "\n",
    "        for i in tqdm(range(T_s // B)):\n",
    "            v = inputs[i * B: (i + 1) * B, :]\n",
    "\n",
    "            w_mul_v = w @ v.T \n",
    "            o = w_mul_v / (np.linalg.norm(w, axis=1).reshape(-1, 1) * np.linalg.norm(v, axis=1))\n",
    "\n",
    "            c = ((1 - s) ** 2) / (1 - o)\n",
    "            \n",
    "            c_sort = np.argsort(c, axis=0)\n",
    "\n",
    "            wins = c_sort[N - 1]\n",
    "\n",
    "            win_mask = np.zeros((N, B))\n",
    "            win_mask[wins, np.arange(B)] = 1\n",
    "            win_mask = (win_mask / np.maximum(np.sum(win_mask, axis=1), 1).reshape(-1, 1))\n",
    "\n",
    "            win_avg = (np.sum(w_mul_v * win_mask, axis=1)).reshape(-1, 1)\n",
    "\n",
    "            v_update = win_mask @ v\n",
    "\n",
    "            del_syn = (v_update - (win_avg * w)) * (((1 - s) ** 2) + 0.1) * xi\n",
    "            \n",
    "            repelled = c_sort[N - 1 - k]\n",
    "            \n",
    "            repel_mask = np.zeros((N, B))\n",
    "            repel_mask[repelled, np.arange(B)] = 1\n",
    "            repel_mask = (repel_mask / np.maximum(np.sum(repel_mask, axis=1), 1).reshape(-1, 1))\n",
    "            repel_mask *= -1 * delta\n",
    "            \n",
    "            repel_avg = (np.sum(w_mul_v * repel_mask, axis=1)).reshape(-1, 1)\n",
    "            \n",
    "            v_repel_update = repel_mask @ v\n",
    "            \n",
    "            del_repel_syn = (v_repel_update - (repel_avg * w)) * ((s ** 2)) * xi\n",
    "            \n",
    "            w += del_syn + del_repel_syn\n",
    "            \n",
    "            if np.amax(np.abs(w)) > 10:\n",
    "                w /= np.array([np.linalg.norm(w, axis=1)]).T\n",
    "            \n",
    "            s[wins] *= (1 - phi)\n",
    "            s += phi * np.sum(o * win_mask, axis=1).reshape(-1, 1)\n",
    "            \n",
    "    print(\"Max val: \", np.amax(s), \"Min value: \", np.amin(s), \"Mean val: \", np.mean(s), \"Std: \", np.std(s))\n",
    "    print(\"Elapsed time: \", time() - start, \" seconds\")\n",
    "    \n",
    "    return (w, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate weights for the final layer based on a quick wta of \n",
    "the final synapses.\n",
    "\"\"\"\n",
    "def gen_final_inits(w, min_val):\n",
    "    w = w.T\n",
    "    \n",
    "    v = flat_x[:T_s]\n",
    "    train_lbls = train_y[:T_s]\n",
    "    \n",
    "    v = v / np.array([np.linalg.norm(v, axis=1)]).T\n",
    "    w = w / np.array([np.linalg.norm(w, axis=1)]).T\n",
    "    \n",
    "    wins = np.argmax(w @ (flat_x[:T_s, :]).T, axis=0)\n",
    "    \n",
    "    n_wins = np.zeros((w.shape[0], 10))\n",
    "    \n",
    "    for (n_i, lbl) in zip(wins, train_lbls):\n",
    "        n_wins[n_i][lbl] += 1\n",
    "        \n",
    "    n_cls = np.argmax(n_wins, axis=1)\n",
    "    \n",
    "    c_o = np.ones((w.shape[0], 10)) * min_val\n",
    "    c_o[np.arange(w.shape[0]), n_cls] = 1\n",
    "    \n",
    "    return c_o\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Network with CompSpec Layer.  Also the final weights are initialized \n",
    "using the wta weight generator.\n",
    "\"\"\"\n",
    "class CompSpecGoodInit(tf.keras.Model):\n",
    "    # The comp_spec_weights have to be transposed from how their originally trained\n",
    "    def __init__(self, comp_spec_weights, final_min_val, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cs_w = tf.constant(comp_spec_weights, dtype='float32')\n",
    "        \n",
    "        self.w = tf.Variable(gen_final_inits(comp_spec_weights, final_min_val), dtype='float32')\n",
    "        \n",
    "    def __call__(self, x, **kwargs):\n",
    "        l1 = tf.matmul(x, self.cs_w)\n",
    "        return tf.matmul(l1, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Network with CompSpec Layer.  The last layer is generated with noise\n",
    "\"\"\"\n",
    "class CompSpecModel(tf.keras.Model):\n",
    "    # The comp_spec_weights have to be transposed from how their originally trained\n",
    "    def __init__(self, comp_spec_weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cs_w = tf.constant(comp_spec_weights, dtype='float32')\n",
    "        \n",
    "        self.w = tf.Variable(tf.random.normal([comp_spec_weights.shape[1], 10]), name='w')\n",
    "        \n",
    "    def __call__(self, x, **kwargs):\n",
    "        l1 = tf.matmul(x, self.cs_w)\n",
    "        return tf.matmul(l1, self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Dialog\n",
    "\n",
    "You want me to say it? I'll say it!  Fuck backprop! Fuck it! Fuck stupid TensorFlow!  Fuck all the fucking companies using these algorithms.  Fuck Gradient Descent!  Fuck it all!  God fucking damn it!\n",
    "\n",
    "Pardon the excessive profanity.  But I really do hate these algorithms.  That being said, CompSpec does do some interesting \n",
    "stuff inside TensorFlow.  \n",
    "\n",
    "I'm not about to retrain all my networks, and I might delete them because they look gross.  ...never mind I won't do that.  I'll just show you the results, talk about why everything is the worst, and them probably go back to my RQI lab and cook up some new algos.  \n",
    "\n",
    "Ok, I'll just copy the code for this first one.  This is trained with 100 CompSpec Neurons, and random last layer initialization.  Here's the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig=plt.figure(figsize=(12,12))\n",
    "\n",
    "mu = 0\n",
    "sig = 1\n",
    "Kx = 10\n",
    "Ky = 10\n",
    "Nep = 1\n",
    "T_s = 60000\n",
    "xi = 0.1\n",
    "phi = 2 / 11\n",
    "\n",
    "B = 100 #Batch size\n",
    "\n",
    "(w, _) = comp_spec_mb(flat_x, S, L, Kx, Ky, Nep, T_s, xi, phi, B)\n",
    "print(\"\\n\\n\")\n",
    "draw_weights(w, Kx, Ky)\n",
    "\n",
    "my_model = CompSpecModel(w.T)\n",
    "\n",
    "my_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "my_model.fit(flat_x, train_y, epochs=20, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only included training with 20 epochs, but I trained this a bunch, so it probably trained on around 100 epochs.  Let's see how it does! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3701 - accuracy: 0.8916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3700554072856903, 0.8916000127792358]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.evaluate(flat_test, test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!  An accuracy of 89%?? That's way better than than the wta classification, right?!\n",
    "\n",
    "You are correct, but that's about where the joy ends.  However, there's one more thing to be joyful about.  With basically every test I've run so far, the test accuracy has been either as good or better than the training accuracy.  So yeah, CompSpec basically is immune to over-fitting (with mnist at least).  That's kinda incredibly dope. \n",
    "\n",
    "Ok, let's get to the part where I hate backprop.  \n",
    "\n",
    "I got ambitious after the success of the 100 CompSpec layer, and I went immediately to 2000 neurons.  Here's the code.  I also probably trained this one on around 50-100 epochs, which literally takes forever.  An epoch takes about 15 seconds for this network so rip me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig=plt.figure(figsize=(12,12))\n",
    "\n",
    "mu = 0\n",
    "sig = 1\n",
    "Kx = 40\n",
    "Ky = 50\n",
    "Nep = 1\n",
    "T_s = 60000\n",
    "xi = 0.1\n",
    "phi = 2 / 11\n",
    "\n",
    "B = 100 #Batch size\n",
    "\n",
    "(w, _) = comp_spec_mb(flat_x, S, L, Kx, Ky, Nep, T_s, xi, phi, B)\n",
    "print(\"\\n\\n\")\n",
    "draw_weights(w, Kx, Ky)\n",
    "\n",
    "two_thousand = CompSpecModel(w.T)\n",
    "\n",
    "two_thousand.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "two_thousand.fit(flat_x, train_y, epochs=20, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how does it perform, you ask?  Look for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 1.0741 - accuracy: 0.8689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0740573406219482, 0.8689000010490417]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_thousand.evaluate(flat_test, test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literally worse than the 100 neuron network!  Are you kidding me?? Also the training accuracy just creeps upward.  It basically plateaued around 87% accuracy.  \n",
    "\n",
    "That makes me angry! And the backprop takes forever!  So I thought this was probably happening because there were simply too many parameters.  So I bumped it down to 400.  Here's the code.  Trained this one for either 100 epochs, or more than 100 epochs.\n",
    "\n",
    "Oh, and I also thought I might try to make this boi's job easier, so I also initialized the final synapses using wta classification.  So basically I gave a neuron a strong weight to it's wta class, and a weak weight to everything else (so that it could still train on those synapses.  We don't want zero gradients everywhere, do we).  I put the new keras model up top, but here's the code for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig=plt.figure(figsize=(12,12))\n",
    "\n",
    "mu = 0\n",
    "sig = 1\n",
    "Kx = 40\n",
    "Ky = 50\n",
    "Nep = 1\n",
    "T_s = 60000\n",
    "xi = 0.1\n",
    "phi = 2 / 11\n",
    "\n",
    "B = 100 #Batch size\n",
    "\n",
    "(w, _) = comp_spec_mb(flat_x, S, L, Kx, Ky, Nep, T_s, xi, phi, B)\n",
    "print(\"\\n\\n\")\n",
    "draw_weights(w, Kx, Ky)\n",
    "\n",
    "fh_good = CompSpecGoodInit(w.T)\n",
    "\n",
    "fh_good.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "fh_good.fit(flat_x, train_y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it does! (*grimaces*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.4050 - accuracy: 0.8832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4050067663192749, 0.8831999897956848]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fh_good.evaluate(flat_test, test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than 2000 gigantor, but still not as good as fucking 100!  Are you kidding me?? \n",
    "\n",
    "I don't know for sure what's happening inside this network, but I'm going to safely blame it on simple accumulation instead of angle between prototypes.  With this setup, you can't really discriminate between things with different structure.  You need to have a way for neurons to express whether something's in line with a particular type of structure.  All of this accumulation business really muddles the details.  \n",
    "\n",
    "With that said, I'm going to go to the conclusion. \n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Man I hate the things I don't make.  Also there's no way in hell I'm going to go dancing through TensorFlow's source code and try to figure out what's going on, and how I can make it faster.  No.  I'm not going to deal with that absolute shit-heap unless I have to.  It's unreal that literally the world is using backprop, given its unthinkably shitty nature.  The biggest \"win\" that I guess can be taken away from this experiment is that CompSpec doesn't overfit.  That's pretty awesome, but I think it's somewhat to be expected.  I'm not trying to minimize some stupid loss function, I'm trying to find structure.  Minimizing a loss function blindly is both where you overfit, and where you build a stupid piece of social media that literally polarizes the world and causes kids to kill themselves.  Fuck minimizing loss functions.  \n",
    "\n",
    "\"What about the benchmarks?\" you ask.  Fuck the benchmarks.  I got some thoughts spinning around the ol' noggerino that I want to try out.  I want to build an architecture that renders stupid backprop obsolete.  That absolutely blows it out of the water. Where computations are run in a local, unthinkably parallelized fashion.  My research is basically bankrolled right now, and I don't want to ruin that by bringing other people into the picture.  I want to run lean in mean.  Come August, if I've failed, I can show the rest of the world what I've done so that I can get into grad school and get some grants and continue my research.  \n",
    "\n",
    "But until then, it's a fucking field day, baby.  I don't want to compare my god-algorithm to Krotov and Hopfield's shitty one.  It's better.  Well, I shouldn't necessarily say that.  It wasn't getting as good of a classification accuracy using tensorflow, but something doesn't add up there.  They must've trained the final layer using something other than my setup, because with their full-on digit prototyping network, they were still getting an accuracy of 98%? I think?  So yeah, something's not adding up.\n",
    "\n",
    "If I need to try to grovel to higher-ups in order to get the resources I need to continue research, so be it.  But I'm not there yet, and I got a lot more time to try to bool.  \n",
    "\n",
    "Oh, I also forgot to mention that I didn't try charged CompSpec.  Why?  Cause fuck that, and fuck backprop.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Flee from back propagation!  That's the most important step.  \n",
    "\n",
    "With regards to benchmarking this algo against KH's paper, I'm going to put that off until it's absolutely necessary.  I don't want to be bogged down with competing with other people.  That's always seemed pretty pointless.  I'd much rather just stay lean and mean, working towards what makes sense to me.  God I'm so glad I'm not in school.  What an unbelievably stupid waste of time.  \n",
    "\n",
    "Now then, it isn't as related to this experiment, but I've been realizing that I need a neuron architecture that allows neurons to react to a wide variety of structures.  Right now, CompSpec learns a couple prototypes.  Great.  But it would be fantastic if each neuron were able to hold information about a variety of structures.  The \"or gate\" functionality that I've been journaling about.  That's what I really need.  And that's what I'm doing next.  \n",
    "\n",
    "God!  Fuck backprop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
