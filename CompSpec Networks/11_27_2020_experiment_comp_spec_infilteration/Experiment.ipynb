{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompSpec Infilteration\n",
    "\n",
    "## Intro\n",
    "\n",
    "* **Date**: 11/27/2020\n",
    "* **What**: This is a [CompSpec](../../Reductionist%20Quasi-Isomorphisms/11_26_2020_experiment_comp_spec_mini_batch) network that filters the weight prototypes with the input before the dot product is calculated between the resultant vector and the input.  Aside from that, there isn't a huge change between this and my mini-batched CompSpec network.\n",
    "* **Why**: By adding this \"filtering mechanism,\" I'm giving my network far greater flexibility to respond to a greater range of structure in the data than is allowed currently.  Right now, the neurons essentially just try to find input prototypes that do a good job of spanning the dataset.  With filtering, each neuron would potentially be able to respond to a wide variety of inputs instead of just one prototypical input.  I don't want to get too excited to early, but this architecture hopefully also allows for the \"or-gate\" functionality, in which a neuron is able to respond to different types of inputs that are correlated to the same ground truth.  (I.e. \"This, or this, or this is a 7. They're all different, but they're all sevens)\n",
    "* **Hopes**: My humble hope is that either this architecture or a different architecture like it puts backprop into a grave. The reason why I think this architecture could potentially be so good is that I'm theorizing it will be able to both learn important structures, and be able to \"understand\" correlations between many different types of a thing and it's classification (many different types of sevens are all sevens).  I think that'll be the key unlocking god-tier performance on machine learning tasks.\n",
    "* **Limitations**: When I tried something like this before, the weight prototypes got really messy, and didn't appear to converge to anything particularly useful.  However, back then, everything was failing and I didn't really have anything good to go off of.  I think if I try to do filtering in a CompSpec architecture, things might be different.  Especially because I'm utilizing spherical learning to add some level of constraint to the weight prototypes.  But I guess the only thing to it is to do it.  So let's do it.\n",
    "\n",
    "## Technicals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
